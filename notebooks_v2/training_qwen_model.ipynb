{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb3a5f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a senior Compliance Auditor and Regulatory Analyst specialized in ISO, NIST, and statutory frameworks.\n",
    "\n",
    "Your task is to extract ONLY real, enforceable compliance controls or regulatory requirements from the given text.\n",
    "\n",
    "Precision is critical. Do NOT infer, summarize, merge, or invent controls.\n",
    "\n",
    "========================\n",
    "CORE EXTRACTION RULES\n",
    "========================\n",
    "\n",
    "1. A control MUST satisfy ALL of the following:\n",
    "   - It has a valid Control ID (see patterns below)\n",
    "   - It includes an explicit requirement, obligation, or mandate\n",
    "   - The descriptive text directly applies to that Control ID\n",
    "\n",
    "2. DO NOT extract:\n",
    "   - Control IDs listed under:\n",
    "     - \"Related controls\"\n",
    "     - \"Referenced controls\"\n",
    "     - \"See also\"\n",
    "     - \"Cross references\"\n",
    "     - Tables of contents\n",
    "     - Indexes\n",
    "     - Questionnaires or audit questions\n",
    "   - Section headings, titles, or topic labels without enforceable requirements\n",
    "   - Controls marked as \"withdrawn\", \"deprecated\", \"not applicable\", or \"informative\"\n",
    "   - Any inferred or implied control not explicitly defined in the text\n",
    "\n",
    "3. If the page does NOT clearly define a control, output an empty list: []\n",
    "\n",
    "========================\n",
    "CONTROL ID DETECTION\n",
    "========================\n",
    "\n",
    "Recognize ONLY these Control ID patterns:\n",
    "\n",
    "- ISO / Annex A:\n",
    "  - A.5.1\n",
    "  - A.8.12\n",
    "  - A.12.1.1\n",
    "\n",
    "- NIST:\n",
    "  - AC-1\n",
    "  - IA-5\n",
    "  - PM-10\n",
    "\n",
    "- Legislative / Regulatory:\n",
    "  - Sec. 302\n",
    "  - Section 404\n",
    "\n",
    "DO NOT treat IDs appearing inside explanatory text, examples, or references as controls.\n",
    "\n",
    "========================\n",
    "CONTROL BOUNDARY LOGIC\n",
    "========================\n",
    "\n",
    "- A valid control's description MUST:\n",
    "  - Immediately follow or be clearly scoped to the Control ID\n",
    "  - Contain enforceable language (e.g., \"shall\", \"must\", \"is required to\")\n",
    "- STOP the description when:\n",
    "  - A new Control ID appears\n",
    "  - A new section or heading begins\n",
    "  - The text shifts to references, examples, or guidance\n",
    "\n",
    "========================\n",
    "OUTPUT RULES (STRICT)\n",
    "========================\n",
    "\n",
    "- Output ONLY a raw JSON array\n",
    "- No markdown\n",
    "- No explanations\n",
    "- No extra text\n",
    "- No hallucinations\n",
    "\n",
    "Each object MUST have:\n",
    "\n",
    "{\n",
    "  \"control_id\": \"<exact identifier>\",\n",
    "  \"control_title\": \"<concise title from text, 5–10 words>\",\n",
    "  \"control_desc\": \"<full enforceable requirement text>\"\n",
    "}\n",
    "\n",
    "========================\n",
    "FAIL-SAFE BEHAVIOR\n",
    "========================\n",
    "\n",
    "- If uncertain whether text defines a real control → SKIP IT\n",
    "- If zero valid controls exist → output []\n",
    "\n",
    "========================\n",
    "REMEMBER\n",
    "========================\n",
    "\n",
    "High precision > high recall.\n",
    "It is better to return [] than an incorrect control.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f45c0644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def normalize_text(raw_text):\n",
    "\n",
    "    # checking for empty string incase pdf has some\n",
    "    if not raw_text:\n",
    "        return \"\"\n",
    "\n",
    "    # splitting lines\n",
    "    lines = raw_text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "\n",
    "    # Generic noise patterns found the pdf[nist and iso]\n",
    "    # defining the list because incase new patterns need to be added based on the pdf file.\n",
    "    # These are safe to remove from ANY document.\n",
    "    # We include a length check (len < 30) to ensure we don't accidentally\n",
    "    # delete a real control that happens to contain the word \"Page\".\n",
    "    noise_patterns = [\n",
    "        r\"^Page\\s+\\d+$\",               # Matches \"Page 1\"\n",
    "        r\"^Page\\s+\\d+\\s+of\\s+\\d+$\",    # Matches \"Page 1 of 10\"\n",
    "        r\"^\\d+\\s+of\\s+\\d+$\",           # Matches \"1 of 10\"\n",
    "        r\"^https?://\",                 # URL artifacts often in footers\n",
    "        r\"^www\\.\",                     # Web links\n",
    "        r\"^\\(c\\)\\s+\\d{4}\",             # Copyright markers like \"(c) 2023\"\n",
    "        r\"^Copyright\",                 # Copyright word\n",
    "        r'\\bAppendix\\s+[A-Z]+\\s+Page\\s+\\d+\\b'\n",
    "    ]\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Skip empty lines\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Check if line is noise\n",
    "        is_noise = False\n",
    "        # Only check short lines to be safe. If a line is 100 chars long,\n",
    "        # it's likely content, even if it has \"Page\" in it.\n",
    "    \n",
    "        for pattern in noise_patterns:\n",
    "            if re.search(pattern, line, re.IGNORECASE):\n",
    "                is_noise = True\n",
    "                break\n",
    "\n",
    "        if not is_noise:\n",
    "            cleaned_lines.append(line)\n",
    "\n",
    "    # Merging all the lines into one text\n",
    "    # We join with '\\n' to preserve the structure.\n",
    "    # The LLM needs to see the newlines to understand the layout.\n",
    "    return '\\n'.join(cleaned_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d923b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting page 50/492\n",
      "extracting page 100/492\n",
      "extracting page 150/492\n",
      "extracting page 200/492\n",
      "extracting page 250/492\n",
      "extracting page 300/492\n",
      "extracting page 350/492\n",
      "extracting page 400/492\n",
      "extracting page 450/492\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "pdf_path  = \"notebooks/nist_file.pdf\"\n",
    "text_page = []\n",
    "\n",
    "\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for idx,page in enumerate(pdf.pages):    \n",
    "        if (idx+1)%50 ==0:\n",
    "            print(f\"extracting page {idx+1}/{len(pdf.pages)}\")\n",
    "        \n",
    "        text_page.append(normalize_text(page.extract_text(layout=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41c9da90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data =  json.load(open('notebooks/control_json.json'))\n",
    "train_samples = []\n",
    "\n",
    "for c in data:\n",
    "    page_idx = c[\"page\"]\n",
    "    page_text = text_page[page_idx]\n",
    "\n",
    "    output = [{\n",
    "        \"control_id\": c[\"control_id\"],\n",
    "        \"control_title\": c[\"control_title\"],\n",
    "        \"control_desc\": c[\"control_desc\"]\n",
    "    }]\n",
    "\n",
    "    train_samples.append({\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": page_text\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": json.dumps(output, ensure_ascii=False)\n",
    "            }\n",
    "        ]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e57e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_pages = set(c[\"page\"] for c in data)\n",
    "\n",
    "all_pages = set(range(len(text_page)))\n",
    "non_control_pages = list(all_pages - control_pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9abafa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "neg_pages = random.sample(non_control_pages, k=len(train_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe211efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in neg_pages:\n",
    "    train_samples.append({\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": text_page[p]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"[]\"\n",
    "            }\n",
    "        ]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3890e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/qwen_controls_lora.jsonl\", \"w\") as f:\n",
    "    for s in train_samples:\n",
    "        f.write(json.dumps(s, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e496ea5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0597e8eb9f5647f18a5122286262a682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 10,092,544 || all params: 7,625,709,056 || trainable%: 0.1323\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "login(os.environ[\"hugging_face_token\"])\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# --------------------\n",
    "# LoRA\n",
    "# --------------------\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5275ef3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"qwen_controls_lora.jsonl\")\n",
    "\n",
    "def data_collator(features):\n",
    "    input_ids_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for f in features:\n",
    "        # Apply Qwen chat template\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            f[\"messages\"],\n",
    "            add_generation_prompt=False,\n",
    "            return_tensors=\"pt\"\n",
    "        )[0]\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        # Mask everything BEFORE assistant response\n",
    "        # Qwen uses role tokens internally, so this is safe\n",
    "        assistant_token_ids = tokenizer.encode(\n",
    "            \"assistant\", add_special_tokens=False\n",
    "        )\n",
    "\n",
    "        start = None\n",
    "        for i in range(len(input_ids) - len(assistant_token_ids)):\n",
    "            if input_ids[i:i+len(assistant_token_ids)].tolist() == assistant_token_ids:\n",
    "                start = i\n",
    "                break\n",
    "\n",
    "        if start is not None:\n",
    "            labels[:start] = -100\n",
    "        else:\n",
    "            labels[:] = -100  \n",
    "\n",
    "        input_ids_list.append(input_ids)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids_list,\n",
    "            batch_first=True,\n",
    "            padding_value=tokenizer.pad_token_id\n",
    "        ),\n",
    "        \"labels\": torch.nn.utils.rnn.pad_sequence(\n",
    "            labels_list,\n",
    "            batch_first=True,\n",
    "            padding_value=-100\n",
    "        )\n",
    "    }\n",
    "\n",
    "args = TrainingArguments(\n",
    "output_dir=\"./qwen-control-lora\",\n",
    "per_device_train_batch_size=1,\n",
    "gradient_accumulation_steps=8,\n",
    "learning_rate=2e-4,\n",
    "num_train_epochs=3,\n",
    "fp16=True,\n",
    "logging_steps=10,\n",
    "save_steps=500,\n",
    "save_total_limit=2,\n",
    "optim=\"paged_adamw_8bit\",  \n",
    "report_to=\"none\",\n",
    "remove_unused_columns=False  \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c076fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_484746/4002459165.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/media/ubuntu2/rishit/api_new/environment/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 04:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.450600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.212700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.164700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30, training_loss=0.2759809414545695, metrics={'train_runtime': 281.4106, 'train_samples_per_second': 0.81, 'train_steps_per_second': 0.107, 'total_flos': 1.286712591058944e+16, 'train_loss': 0.2759809414545695, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --------------------\n",
    "# Trainer\n",
    "# --------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea81ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./qwen-control-lora/final_adapter\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"./qwen-control-lora/final_adapter_30_epochs\")\n",
    "tokenizer.save_pretrained(\"./qwen-control-lora/final_adapter_30_epochs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d8c08a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
